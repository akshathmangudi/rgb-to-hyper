# rgb-to-hyper
end-to-end implementation of microplastics detection in water using hyperspectral imaging.

# How it works

Summary of the Training and Prediction Flow

A. Training Flow (mode = "global"):

1. Model and Optimizer Initialization:
   - Instantiate Generator and Discriminator.
   - Initialize their respective Adam optimizers.

2. Logging Setup:
   - Create a summary_writer for TensorBoard to log training metrics.

3. Checkpoint Path Determination:
     - Set checkpoint_path to 'global_ckpt' for global training.

4. Training Execution:
   - Call train_gan with all necessary parameters.
   - Inside train_gan:
     - Checkpoint Restoration: Load existing checkpoints if available.
     - Data Loading and Preparation: Load and preprocess paired RGB and HSI images.
     - Epoch Loop: For each epoch, shuffle data and iterate over batches.
       - Batch Processing: Perform augmentation, train discriminator, train generator, compute metrics, and log progress.
       - Checkpoint Saving: Save model states at the end of each epoch.
     - Post-Training: Save final metrics and optionally generate sample outputs.


B. Prediction Flow (mode = "predict")

1. Checkpoint Restoration:
  - Use load_model_and_predict to load the Generator model from the 'global_ckpt' directory.

2. Data Loading:
   - Load RGB images designated for prediction.

3. HSI Generation:
  - Use the restored Generator to create HSI images from the RGB inputs.

4. Output Saving:
   - Save the generated HSI images as TIFF files in the specified directory.

# Underlying Mathematics of the Model

The project utilizes a **Generative Adversarial Network (GAN)** framework, specifically tailored to convert RGB images to HyperSpectral Imaging (HSI) data. Below is a breakdown of the mathematical concepts and components that underpin the model.

### 1. Generative Adversarial Networks (GANs)

**GANs** consist of two neural networks, the **Generator** and the **Discriminator**, which are trained simultaneously through adversarial processes.

- **Generator (\(G\))**:
  - **Purpose**: Generates synthetic HSI images from input RGB images.
  - **Mathematical Objective**: \(G: \mathbb{R}^{H \times W \times 3} \rightarrow \mathbb{R}^{H \times W \times C}\), where \(H\) and \(W\) are image dimensions, and \(C\) is the number of HSI channels.

- **Discriminator (\(D\))**:
  - **Purpose**: Differentiates between real HSI images and those generated by \(G\).
  - **Mathematical Objective**: \(D: \mathbb{R}^{H \times W \times C} \rightarrow [0,1]\), outputting the probability that an input image is real.

**Adversarial Loss**:
The Generator and Discriminator engage in a minimax game with the following objective functions:

- **Discriminator Loss (\( \mathcal{L}_D \))**:
  \[
  \mathcal{L}_D = -\mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] - \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
  \]

- **Generator Loss (\( \mathcal{L}_G \))**:
  \[
  \mathcal{L}_G = -\mathbb{E}_{z \sim p_z}[\log D(G(z))]
  \]

**Wasserstein GAN with Gradient Penalty (WGAN-GP)**:
To improve training stability, the model incorporates WGAN-GP, which modifies the loss functions to enforce the Lipschitz constraint using a gradient penalty.

- **Gradient Penalty (\( \mathcal{L}_{GP} \))**:
  \[
  \mathcal{L}_{GP} = \lambda \mathbb{E}_{\hat{x} \sim P_{\hat{x}}} \left[ (\|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2 \right]
  \]
  where \( \hat{x} \) are samples interpolated between real and generated data, and \( \lambda \) is the penalty coefficient.

- **Modified Discriminator Loss**:
  \[
  \mathcal{L}_D = \mathbb{E}_{x \sim p_{\text{data}}}[D(x)] - \mathbb{E}_{z \sim p_z}[D(G(z))] + \mathcal{L}_{GP}
  \]

### 2. Spectral Normalization

**Spectral Normalization** is applied to the Discriminator to stabilize training by constraining the Lipschitz constant of each layer.

- **Mathematical Basis**:
  \[
  \hat{W} = \frac{W}{\sigma(W)}
  \]
  where \( \sigma(W) \) is the largest singular value of weight matrix \( W \).

- **Implementation**:
  Iteratively approximates the dominant singular vectors using power iterations to compute \( \sigma(W) \).

### 3. Loss Functions

Beyond adversarial losses, the model incorporates several auxiliary loss functions to enhance the quality and fidelity of generated HSI images.

- **Mean Squared Error (MSE)**:
  \[
  \text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
  \]
  Measures the average squared difference between the generated and real HSI pixels.

- **Peak Signal-to-Noise Ratio (PSNR)**:
  \[
  \text{PSNR} = 20 \cdot \log_{10}\left(\frac{\text{MAX}_I}{\sqrt{\text{MSE}}}\right)
  \]
  where \( \text{MAX}_I \) is the maximum possible pixel value. PSNR assesses the quality of reconstruction.

- **Spectral Angle Mapper (SAM)**:
  \[
  \text{SAM}(y, \hat{y}) = \arccos\left( \frac{y \cdot \hat{y}}{\|y\| \|\hat{y}\|} \right)
  \]
  Measures the spectral similarity between two spectra \( y \) and \( \hat{y} \), providing angular differences in high-dimensional space.

- **Perceptual Loss**:
  Utilizes a pre-trained VGG19 network to compute the loss in a feature space, capturing high-level perceptual differences.
  \[
  \mathcal{L}_{\text{perceptual}} = \sum_{i} \| \phi_i(y) - \phi_i(\hat{y}) \|_2^2
  \]
  where \( \phi_i \) represents the activation maps from layer \( i \) of VGG19.

### 4. Training Objectives

The overall objective combines adversarial and auxiliary losses to train the Generator and Discriminator.

- **Generator Objective**:
  \[
  \mathcal{L}_G^{\text{total}} = \mathcal{L}_G + \lambda_{\text{pixel}} \cdot \mathcal{L}_{\text{MSE}} + \lambda_{\text{perceptual}} \cdot \mathcal{L}_{\text{perceptual}}
  \]
  Balances adversarial objectives with pixel-wise and perceptual fidelity.

- **Discriminator Objective**:
  \[
  \mathcal{L}_D^{\text{total}} = \mathcal{L}_D
  \]
  Primarily focuses on differentiating real from generated data with the gradient penalty for stability.

### 5. Optimization

Both networks are optimized using gradient-based methods, typically Adam optimizer, updating weights to minimize their respective loss functions.

- **Parameter Updates**:
  \[
  \theta_D \leftarrow \theta_D - \eta \cdot \nabla_{\theta_D} \mathcal{L}_D^{\text{total}}
  \]
  \[
  \theta_G \leftarrow \theta_G - \eta \cdot \nabla_{\theta_G} \mathcal{L}_G^{\text{total}}
  \]
  where \( \eta \) is the learning rate.

### 6. Evaluation Metrics

Post-training, the model's performance is evaluated using the aforementioned metrics (MSE, PSNR, SAM) to quantify the quality of generated HSI images.

- **Average Metrics Calculation**:
  \[
  \text{Average Metric} = \frac{\sum_{i=1}^{N} \text{Metric}_i}{N}
  \]
  Aggregates performance across all batches and epochs.

### 7. Additional Considerations

- **Data Augmentation**:
  Enhances model generalization by applying transformations like rotations, flips, and scaling to the training data.

- **Checkpointing**:
  Saves model states periodically to allow resumption and prevent loss of progress due to interruptions.

- **Logging**:
  Records training progress and metrics for monitoring and debugging purposes.

### Summary

The model integrates advanced GAN architectures with specialized loss functions and normalization techniques to effectively translate RGB images into high-fidelity HSI data. The mathematical framework ensures stability during training, preserves spectral integrity, and evaluates model performance comprehensively.
